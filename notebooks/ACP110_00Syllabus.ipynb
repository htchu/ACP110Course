{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ACP110_00Syllabus.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 110-2 W0 Syllabus\n",
        "__Advanced Computer Programming in Python__\n",
        "\n"
      ],
      "metadata": {
        "id": "qB57djOIXRSM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Course Schedule\n",
        "* W1-課程介紹/Introduction\n",
        "* W2-Python urllib \n",
        "* W3-BeautifulSoup\n",
        "* W4-Web Crawlers\n",
        "* W5-Scrapy\n",
        "* W6-Storing Data\n",
        "* W7-Flask Routes\n",
        "* W8-Jinja template \n",
        "* W9-Midterm presentation\n",
        "* W10-Flask-Mail\n",
        "* W11-REST API\n",
        "* W12-AWS Lambda + S3 (1) \n",
        "* W13-AWS Lambda + S3 (1)\n",
        "* W14-AWS Glue Data \n",
        "* W15-AWS Step Functions\n",
        "* W16-AWS Elastic Beanstalk\n",
        "* W17-Sample app-Book Recommender \n",
        "* W18-Final presentation\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EqhFHnRdIO5W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reference books (O’Reilly Media)\n",
        "\n",
        "* Web Scraping with Python (Second Edition)-Collecting More Data from the Modern Web by Ryan Mitchell\n",
        "![Web Scraping with Python](https://kbimages1-a.akamaihd.net/b08563e1-d7e3-4a87-9115-225f270d0441/353/569/90/False/web-scraping-with-python-3.jpg)\n",
        "\n",
        "英文版: https://www.oreilly.com/library/view/web-scraping-with/9781491985564/\n",
        "\n",
        "中文版: https://www.tenlong.com.tw/products/9789864769261\n",
        "\n",
        "Github: https://github.com/REMitchell/python-scraping\n",
        "\n",
        "* Flask Web Development(Second Edition): Developing Web Applications With Python by Grinberg, \n",
        "![Flask Web Development](https://im2.book.com.tw/image/getImage?i=https://www.books.com.tw/img/F01/410/81/F014108175.jpg&v=609a97b1&w=348&h=348)\n",
        "\n",
        "英文版: https://www.oreilly.com/library/view/flask-web-development/9781491991725/\n",
        "\n",
        "英文版: https://www.books.com.tw/products/F014108175\n",
        "\n",
        "中文版: https://www.books.com.tw/products/0010793455\n",
        "\n",
        "Github: https://github.com/miguelgrinberg/flasky\n"
      ],
      "metadata": {
        "id": "uP9m_r8WCGY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Web Scraping with Python \n",
        "* 第一部 建構擷取程序\n",
        "** 第一章 你的第一個擷取程序\n",
        "** 第二章 進階HTML解析\n",
        "** 第三章 撰寫網站爬行程序\n",
        "** 第四章 網站爬行模型\n",
        "** 第五章 Scrapy\n",
        "** 第六章 儲存資料\n",
        "* 第二部 儲存資料\n",
        "** 第七章 讀取文件\n",
        "** 第八章 清理髒資料\n",
        "** 第九章 讀寫自然語言\n",
        "** 第十章 表單與登入\n",
        "** 第十一章 與擷取相關的JavaScript\n",
        "** 第十二章 透過API 爬行\n",
        "** 第十三章 影像處理與文字辨識\n",
        "** 第十四章 避開擷取陷阱\n",
        "** 第十五章 以爬行程序測試你的網站\n",
        "** 第十六章 平行擷取網站\n",
        "** 第十七章 遠端擷取\n",
        "** 第十八章 網站擷取的法規與道德"
      ],
      "metadata": {
        "id": "a-cmjm2qsFSD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reference webpages\n"
      ],
      "metadata": {
        "id": "I6Ty-C7dFhMv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1-網頁抓取(Web scraping) & 網絡爬蟲（web crawler）\n",
        "\n",
        "網頁抓取是一種從網頁上取得頁面內容的電腦軟件技術。通常透過軟件使用低階別的超文字傳輸協定模仿人類的正常存取。\n",
        "網絡爬蟲（web crawler）是一種用來自動瀏覽萬維網的網絡機械人。網絡爬蟲可以將自己所存取的頁面儲存下來，以便搜尋引擎事後生成索引供用戶搜尋。爬蟲存取網站的過程會消耗目標系統資源。不少網絡系統並不默許爬蟲工作。因此在存取大量頁面時，爬蟲需要考慮到規劃、負載，還需要講「禮貌」。 不願意被爬蟲存取、被爬蟲主人知曉的公開站點可以使用robots.txt檔案之類的方法避免存取。這個檔案可以要求機械人只對網站的一部分進行索引，或完全不作處理。\n",
        "\n",
        "Web scraping is data scraping used for extracting data from websites."
      ],
      "metadata": {
        "id": "DRGGMjkB5cZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2-Web apps with Flask(用Python開發Web應用程式)\n",
        "\n",
        "使用Python微框架Flask並充分運用創造力來建構Web應用程式！藉由作者Miguel Grinberg開發的完整app，你會從基礎開始學習Flask。全面修改後的第二版加入了過去三年來重要的技術沿革。\n",
        "\n",
        "\n",
        "你可以從中學到這個框架的核心功能，並且運用進階的網路技術(例如資料庫遷移和應用程式開發介面)來擴充app。本書會在每一章的第一個部分做主題與背景的介紹，在第二個部分帶著你實際操作。\n",
        "\n",
        " \n",
        "包含三個部分：\n",
        "\n",
        "* 詳細介紹Flask：說明以Flask開發web app的基本知識，以及適合中、大型app的app架構\n",
        "\n",
        "* 組建Flasky：逐步說明如何重複使用模板、為清單分頁、以及如何使用豐富文字來建立開放原始碼的部落格app\n",
        "\n",
        "* 完成最後一里路：深入探討Flask app的單元測試策略、效能分析技術，以及部署選項"
      ],
      "metadata": {
        "id": "AmraskNsEZ5L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flask Web Development : Developing Web Applications with Python\n",
        "* 第一部分 詳細介紹 Flask\n",
        "** 第一章 安裝\n",
        "** 第二章 基本 app 結構\n",
        "** 第三章 模板 \n",
        "** 第四章 web 表單\n",
        "** 第五章 資料庫\n",
        "** 第六章 Email \n",
        "** 第七章 大型的 app 結構\n",
        "* 第二部分 範例：社群部落格 app\n",
        "** 第八章 使用者身分驗證\n",
        "** 第九章 使用者角色 \n",
        "** 第十章 使用者個人資訊 \n",
        "** 第十一章 部落格文章\n",
        "** 第十二章 追隨者 \n",
        "** 第十三章 使用者評論 \n",
        "** 第十四章 應用程式開發介面\n",
        "* 第三部分 最後一哩路\n",
        "** 第十五章 測試\n",
        "** 第十六章 效能 \n",
        "** 第十七章 部署\n",
        "** 第十八章 其他的資源"
      ],
      "metadata": {
        "id": "9BOwfZ2QHqRi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3-AWS Scaling up a Serverless Web Crawler\n",
        "\n",
        "https://aws.amazon.com/tw/blogs/architecture/scaling-up-a-serverless-web-crawler-and-search-engine/\n",
        "\n",
        "![Serverless Web Crawler](https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2021/02/15/fig3-overall-architecture.png)\n"
      ],
      "metadata": {
        "id": "NBZ0ioH53yhS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AWS references\n",
        "\n",
        "* Build and automate a modern serverless data lake on AWS\n",
        "https://aws.amazon.com/tw/blogs/big-data/build-and-automate-a-serverless-data-lake-using-an-aws-glue-trigger-for-the-data-catalog-and-etl-jobs/\n",
        "\n"
      ],
      "metadata": {
        "id": "9vEAzvly4LKz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tools\n",
        "* PythonAnywhere: Host, run, and code Python in the cloud\n",
        " > https://www.pythonanywhere.com/\n",
        "* AWS\n",
        " > https://aws.amazon.com/tw/free/\n",
        "* AWS academy\n",
        " > https://www.awsacademy.com/LMS_Login"
      ],
      "metadata": {
        "id": "P0zA9KrIJKPU"
      }
    }
  ]
}